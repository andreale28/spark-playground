{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:49:49.995483Z",
     "start_time": "2023-10-18T13:49:49.785977Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast, json\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, udf, split, regexp_replace\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.dataframe import  DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: sql.sql.extensions\n",
      "23/10/18 20:49:51 WARN Utils: Your hostname, Andrea-Le-MBP-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.11 instead (on interface en0)\n",
      "23/10/18 20:49:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/sonle/.sdkman/candidates/spark/3.4.0/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/sonle/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/sonle/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9d6964ce-e7ef-4c82-b69b-da86de202a9e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 177ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9d6964ce-e7ef-4c82-b69b-da86de202a9e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "23/10/18 20:49:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = (\n",
    "    SparkSession.builder.appName(\"homework1\")\n",
    "    .config(\"spark.driver.memory\", \"16g\")\n",
    "    .config(\"spark.driver.cores\", 3)\n",
    "    .config(\"sql.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n",
    "    )\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:49:54.646218Z",
     "start_time": "2023-10-18T13:49:49.998549Z"
    }
   },
   "id": "f7f4cea9baa374e9"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Mac\", StringType(), True),\n",
    "    StructField(\"SessionMainMenu\", StringType(), True),\n",
    "    StructField(\"LogId\", StringType(), True),\n",
    "    StructField(\"Event\", StringType(), True),\n",
    "    StructField(\"ItemId\", StringType(), True),\n",
    "    StructField(\"RealTimePlaying\", StringType(), True)\n",
    "])\n",
    "log_path = \"/Users/sonle/Documents/GitHub/spark-playground/data/DataSampleTest/logt21.txt\"\n",
    "user_path = \"/Users/sonle/Documents/GitHub/spark-playground/data/DataSampleTest/user_info.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:49:54.658262Z",
     "start_time": "2023-10-18T13:49:54.650663Z"
    }
   },
   "id": "d2c223265c5db9c5"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def read_data(log_paths: str, user_info_path: str, log_schema: StructType) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Input the log_path for log user data and user_info_path for user info. Log path must be a glob pattern\n",
    "    For log user data, an additional dict_to_json is needed to revalidate the json strings\n",
    "    For user info data, small modification is needed to get 2-columns format\n",
    "    :param user_info_path: strings, a path for user info data\n",
    "    :param log_paths: strings, a path for log user data\n",
    "    :param log_schema: StrucType schema of log_data\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    dict_to_json = udf(lambda x: json.dumps(ast.literal_eval(x)))\n",
    "    #log data\n",
    "    log_df = (\n",
    "        spark.read.text(paths=log_path, lineSep='\\n')\n",
    "        .withColumn(\"data\", from_json(dict_to_json(\"value\"), schema=schema, options={\"encoding\": \"utf-8\"}))\n",
    "        .select(\"data.*\")\n",
    "    )\n",
    "    #user info data\n",
    "    user_df = (\n",
    "        spark.read.text(paths=user_info_path)\n",
    "        .withColumn(\"Mac\", split(col(\"value\"), \"\\t\")[0])\n",
    "        .withColumn(\"Days\", split(col(\"value\"), \"\\t\")[1])\n",
    "        .select(\n",
    "            regexp_replace(col(\"Mac\"), \"^.{4}\", \"\").cast(\"string\").alias(\"MacId\"),\n",
    "            col(\"Days\").cast(\"integer\")\n",
    "        )\n",
    "        .filter(col(\"Days\").isNotNull())  #we need this line to remove the first row (MAC, #days)\n",
    "    )\n",
    "    \n",
    "    df = log_df.join(\n",
    "        user_df,\n",
    "        log_df.Mac == user_df.MacId,\n",
    "        'left'\n",
    "    ).select(\n",
    "        \"Mac\", \"SessionMainMenu\", \"LogId\", \"Event\", \"ItemId\", \"RealTimePlaying\", \"Days\"\n",
    "    )\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:49:54.671680Z",
     "start_time": "2023-10-18T13:49:54.656054Z"
    }
   },
   "id": "315a89526d933aa3"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----+------------+---------+---------------+----+\n",
      "|         Mac|     SessionMainMenu|LogId|       Event|   ItemId|RealTimePlaying|Days|\n",
      "+------------+--------------------+-----+------------+---------+---------------+----+\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   52|     StopVOD|100052388|          570.3| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   40|   EnterIPTV|     null|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   55|     NextVOD|100052388|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   18|ChangeModule|     null|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   54|     PlayVOD|100052388|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   40|   EnterIPTV|     null|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   55|     NextVOD|100052388|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   52|     StopVOD|100052388|         3384.6| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   40|   EnterIPTV|     null|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   52|     StopVOD|100052388|          621.9| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   18|ChangeModule|     null|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   12|     Standby|     null|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   51|    StartVOD|100052388|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   51|    StartVOD|100000148|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   42| StopChannel|      158|          6.657| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   42| StopChannel|       52|         11.327| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   18|ChangeModule|     null|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   41|StartChannel|      125|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   52|     StopVOD|100052388|         1158.6| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   40|   EnterIPTV|     null|           null| 375|\n",
      "+------------+--------------------+-----+------------+---------+---------------+----+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = read_data(log_path, user_path, log_schema=schema)\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:50:00.857851Z",
     "start_time": "2023-10-18T13:49:54.673539Z"
    }
   },
   "id": "de0757ff148c3ac"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----+------------+---------+---------------+----+\n",
      "|         Mac|     SessionMainMenu|LogId|       Event|   ItemId|RealTimePlaying|Days|\n",
      "+------------+--------------------+-----+------------+---------+---------------+----+\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   52|     StopVOD|100052388|          570.3| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   40|   EnterIPTV|     null|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   55|     NextVOD|100052388|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   18|ChangeModule|     null|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   54|     PlayVOD|100052388|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   40|   EnterIPTV|     null|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   55|     NextVOD|100052388|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   52|     StopVOD|100052388|         3384.6| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   40|   EnterIPTV|     null|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   52|     StopVOD|100052388|          621.9| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   18|ChangeModule|     null|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   12|     Standby|     null|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   51|    StartVOD|100052388|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   51|    StartVOD|100000148|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   42| StopChannel|      158|          6.657| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   42| StopChannel|       52|         11.327| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   18|ChangeModule|     null|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   41|StartChannel|      125|           null| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   52|     StopVOD|100052388|         1158.6| 375|\n",
      "|B046FCAC0DC1|B046FCAC0DC1:2016...|   40|   EnterIPTV|     null|           null| 375|\n",
      "+------------+--------------------+-----+------------+---------+---------------+----+\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"Mac\").isin(\"B046FCAC0DC1\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:02:26.532265Z",
     "start_time": "2023-10-18T14:02:25.928459Z"
    }
   },
   "id": "41b4650631dd88a9"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:50:05.781071Z",
     "start_time": "2023-10-18T13:50:05.772246Z"
    }
   },
   "id": "50d848f426f79a01"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
