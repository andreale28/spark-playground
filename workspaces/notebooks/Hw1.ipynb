{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-21T08:07:30.524379Z",
     "start_time": "2023-10-21T08:07:30.469446Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "from typing import Tuple\n",
    "\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import from_json, col, udf, split, regexp_replace, regexp_extract, to_timestamp, to_date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: sql.sql.extensions\n",
      "23/10/21 14:18:57 WARN Utils: Your hostname, Andrea-Le-MBP-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.9 instead (on interface en0)\n",
      "23/10/21 14:18:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/sonle/.sdkman/candidates/spark/3.4.0/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/sonle/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/sonle/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-da2e2fed-bb57-454c-8204-1bdf835d6d5c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 237ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-da2e2fed-bb57-454c-8204-1bdf835d6d5c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "23/10/21 14:19:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = (\n",
    "    SparkSession.builder.appName(\"homework1\")\n",
    "    .config(\"spark.driver.memory\", \"16g\")\n",
    "    .config(\"spark.driver.cores\", 3)\n",
    "    .config(\"sql.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n",
    "    )\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T07:19:20.643411Z",
     "start_time": "2023-10-21T07:18:55.320770Z"
    }
   },
   "id": "f7f4cea9baa374e9"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Mac\", StringType(), True),\n",
    "    StructField(\"SessionMainMenu\", StringType(), True),\n",
    "    StructField(\"LogId\", StringType(), True),\n",
    "    StructField(\"Event\", StringType(), True),\n",
    "    StructField(\"ItemId\", StringType(), True),\n",
    "    StructField(\"RealTimePlaying\", StringType(), True)\n",
    "])\n",
    "log_dir = \"/Users/sonle/Documents/GitHub/spark-playground/data/DataSampleTest/logt21.txt\"\n",
    "user_dir = \"/Users/sonle/Documents/GitHub/spark-playground/data/DataSampleTest/user_info.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T07:19:20.661237Z",
     "start_time": "2023-10-21T07:19:20.648115Z"
    }
   },
   "id": "d2c223265c5db9c5"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "def read_data(log_paths: str, user_info_path: str, log_schema: StructType) -> tuple[DataFrame, DataFrame, DataFrame]:\n",
    "    \"\"\"\n",
    "    Input the log_path for log user data and user_info_path for user info. Log path must be a glob pattern\n",
    "    For log user data, an additional dict_to_json is needed to revalidate the json strings\n",
    "    For user info data, small modification is needed to get 2-columns format\n",
    "    :param user_info_path: strings, a path for user info data\n",
    "    :param log_paths: strings, a path for log user data\n",
    "    :param log_schema: StrucType schema of log_data\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    dict_to_json = udf(lambda x: json.dumps(ast.literal_eval(x)))\n",
    "    #log data\n",
    "    log_df = (\n",
    "        spark.read.text(paths=log_paths, lineSep='\\n')\n",
    "        .withColumn(\"data\", from_json(dict_to_json(\"value\"), schema=schema, options={\"encoding\": \"utf-8\"}))\n",
    "        .select(\"data.*\")\n",
    "        .withColumn(\"SessionMainMenu\",\n",
    "                    to_timestamp(regexp_replace(col(\"SessionMainMenu\"),  r\"^(\\w+):\", \"\"),\n",
    "                            \"yyyy:MM:dd:HH:mm:ss:SSS\"))\n",
    "        \n",
    "    )\n",
    "    #user info data\n",
    "    user_df = (\n",
    "        spark.read.text(paths=user_info_path)\n",
    "        .withColumn(\"Mac\", split(col(\"value\"), \"\\t\")[0])\n",
    "        .withColumn(\"Days\", split(col(\"value\"), \"\\t\")[1])\n",
    "        .select(\n",
    "            regexp_replace(col(\"Mac\"), \"^.{4}\", \"\").cast(\"string\").alias(\"MacId\"),\n",
    "            col(\"Days\").cast(\"integer\")\n",
    "        )\n",
    "        .filter(col(\"Days\").isNotNull())  #we need this line to remove the first row (MAC, #days)\n",
    "    )\n",
    "    \n",
    "    df = log_df.join(\n",
    "        user_df,\n",
    "        log_df.Mac == user_df.MacId,\n",
    "        'left'\n",
    "    ).select(\n",
    "        \"Mac\", \"SessionMainMenu\", \"LogId\", \"Event\", \"ItemId\", \"RealTimePlaying\", \"Days\"\n",
    "    )\n",
    "    return df, user_df, log_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T08:10:09.644476Z",
     "start_time": "2023-10-21T08:10:09.574566Z"
    }
   },
   "id": "315a89526d933aa3"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----+------------+---------+---------------+----+\n",
      "|         Mac|     SessionMainMenu|LogId|       Event|   ItemId|RealTimePlaying|Days|\n",
      "+------------+--------------------+-----+------------+---------+---------------+----+\n",
      "|B046FCAC0DC1|2016-02-12 12:35:...|   52|     StopVOD|100052388|          570.3| 375|\n",
      "|B046FCAC0DC1|2016-02-11 01:01:...|   40|   EnterIPTV|     null|           null| 375|\n",
      "|B046FCAC0DC1|2016-02-11 01:02:...|   55|     NextVOD|100052388|           null| 375|\n",
      "|B046FCAC0DC1|2016-02-12 04:44:...|   18|ChangeModule|     null|           null| 375|\n",
      "|B046FCAC0DC1|2016-02-12 12:35:...|   54|     PlayVOD|100052388|           null| 375|\n",
      "|B046FCAC0DC1|2016-02-12 04:44:...|   40|   EnterIPTV|     null|           null| 375|\n",
      "|B046FCAC0DC1|2016-02-12 12:35:...|   55|     NextVOD|100052388|           null| 375|\n",
      "|B046FCAC0DC1|2016-02-12 12:35:...|   52|     StopVOD|100052388|         3384.6| 375|\n",
      "|B046FCAC0DC1|2016-02-13 17:25:...|   40|   EnterIPTV|     null|           null| 375|\n",
      "|B046FCAC0DC1|2016-02-14 01:41:...|   52|     StopVOD|100052388|          621.9| 375|\n",
      "|B046FCAC0DC1|2016-02-14 13:48:...|   18|ChangeModule|     null|           null| 375|\n",
      "|B046FCAC0DC1|2016-02-14 01:41:...|   12|     Standby|     null|           null| 375|\n",
      "|B046FCAC0DC1|2016-02-14 01:41:...|   51|    StartVOD|100052388|           null| 375|\n",
      "|B046FCAC0DC1|2016-02-14 01:41:...|   51|    StartVOD|100000148|           null| 375|\n",
      "|B046FCAC0DC1|2016-02-14 13:48:...|   42| StopChannel|      158|          6.657| 375|\n",
      "|B046FCAC0DC1|2016-02-17 18:29:...|   42| StopChannel|       52|         11.327| 375|\n",
      "|B046FCAC0DC1|2016-02-17 00:22:...|   18|ChangeModule|     null|           null| 375|\n",
      "|B046FCAC0DC1|2016-02-17 03:00:...|   41|StartChannel|      125|           null| 375|\n",
      "|B046FCAC0DC1|2016-02-17 03:00:...|   52|     StopVOD|100052388|         1158.6| 375|\n",
      "|B046FCAC0DC1|2016-02-17 00:22:...|   40|   EnterIPTV|     null|           null| 375|\n",
      "+------------+--------------------+-----+------------+---------+---------------+----+\n"
     ]
    }
   ],
   "source": [
    "df, user_df, log_df = read_data(log_dir, user_dir, log_schema=schema)\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T08:10:28.311737Z",
     "start_time": "2023-10-21T08:10:27.784490Z"
    }
   },
   "id": "de0757ff148c3ac"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|       MacId|Days|\n",
      "+------------+----+\n",
      "|B046FCB79E0B|  20|\n",
      "|B046FCB3528B| 181|\n",
      "|B046FCAAFB73| 426|\n",
      "|B046FCAAFB72| 426|\n",
      "|B046FCAA2085| 429|\n",
      "|B046FCAA0669| 380|\n",
      "|B046FCB343BF| 376|\n",
      "|B046FCAC0CFB| 376|\n",
      "|B046FCABED45| 378|\n",
      "|B046FCAD80FC| 305|\n",
      "|B046FCB1E3FE| 255|\n",
      "|B046FCB27666| 210|\n",
      "|B046FCB42341| 142|\n",
      "|B046FCB6D6B2|  46|\n",
      "|B046FCB6D4BC|  46|\n",
      "|B046FCB6D4B6|  46|\n",
      "|B046FCA6A3F4| 583|\n",
      "|B046FCA86BD5| 493|\n",
      "|B046FCABE3BC| 425|\n",
      "|B046FCAC125F| 374|\n",
      "+------------+----+\n"
     ]
    }
   ],
   "source": [
    "user_df.createOrReplaceTempView(\"user_data\")\n",
    "spark.sql(\n",
    "    sqlQuery=\"\"\"\n",
    "    select * from user_data\n",
    "    \"\"\"\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T08:11:13.289162Z",
     "start_time": "2023-10-21T08:11:13.025119Z"
    }
   },
   "id": "41b4650631dd88a9"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:50:05.781071Z",
     "start_time": "2023-10-18T13:50:05.772246Z"
    }
   },
   "id": "50d848f426f79a01"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
