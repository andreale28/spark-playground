{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Homework 2\n",
    "In homework 2, we will work with log content of user in 30 days. The general idea here is we first try to analyze one day\n",
    "then apply it to a month. Thanks to **pyspark**  supporting glob pattern directory, this can be done with ease. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4914d9e67d76fe97"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col, to_date, when, \\\n",
    "\tinput_file_name, regexp_extract, sum, lit"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:55:57.770358Z",
     "start_time": "2023-10-25T13:55:57.348415Z"
    }
   },
   "id": "127052a690f1333b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: sql.sql.extensions\n",
      "23/10/25 20:56:06 WARN Utils: Your hostname, Andrea-Le-MBP-Pro.local resolves to a loopback address: 127.0.2.2; using 192.168.1.9 instead (on interface en0)\n",
      "23/10/25 20:56:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/sonle/.sdkman/candidates/spark/3.4.0/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/sonle/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/sonle/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6477a3f6-0ad4-4b44-9ab1-847172d4cf10;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 194ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6477a3f6-0ad4-4b44-9ab1-847172d4cf10\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/6ms)\n",
      "23/10/25 20:56:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = (\n",
    "\tSparkSession.builder.appName(\"homework1\")\n",
    "\t.config(\"spark.driver.memory\", \"16g\")\n",
    "\t.config(\"spark.driver.cores\", 4)\n",
    "\t.config(\"sql.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "\t.config(\n",
    "\t\t\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n",
    "\t)\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:56:29.067591Z",
     "start_time": "2023-10-25T13:56:04.594824Z"
    }
   },
   "id": "89113c134976b87f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parquet is all the way\n",
    "Log content here is stored on disk in **json format** which is not well-suited for reading purpose. \n",
    "Hence, we should load and write it down as **parquet format** since parquet is more toward to analytical workload. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "768a2d637be73c1a"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "log_path = \"/Users/sonle/Documents/GitHub/spark-playground/data/log_content\"\t\n",
    "data_path = \"/Users/sonle/Documents/GitHub/spark-playground/data/\"\n",
    "parquet_path = \"parquet_log_content.parquet\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:55:44.511831Z",
     "start_time": "2023-10-25T13:55:44.494459Z"
    }
   },
   "id": "f295133acf4acecf"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.path.exists(os.path.join(data_path, \"log_content\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:56:29.078754Z",
     "start_time": "2023-10-25T13:56:29.072034Z"
    }
   },
   "id": "1be8f331bfa18f4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Just run this function one time\n",
    "def read_log_content(\n",
    "\t\tdata_paths: str,\n",
    "\t\tparquet_path: str\n",
    "):\n",
    "\t\"\"\"\n",
    "    Read and rename log content data in json format and \n",
    "    write it to disk in parquet format with partitioning option\n",
    "    :param data_paths: data directory\n",
    "    :param parquet_path: parquet file name\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\treturn (\n",
    "\t\tspark.read.json(f\"{data_paths}/log_content/*.json\")\n",
    "\t\t.select(\n",
    "\t\t\tcol(\"_index\").alias(\"Index\"),\n",
    "\t\t\tcol(\"_type\").alias(\"Type\"),\n",
    "\t\t\tcol(\"_id\").alias(\"Id\"),\n",
    "\t\t\tcol(\"_score\").alias(\"Score\"),\n",
    "\t\t\tcol(\"_source.*\"),\n",
    "\t\t\tto_date(\n",
    "\t\t\t\tregexp_extract(input_file_name(), r\"\\d{8}\", 0),\n",
    "\t\t\t\t\"yyyyMMdd\"\n",
    "\t\t\t).alias(\"Date\")\n",
    "\t\t)\n",
    "\t\t.write.parquet(\n",
    "\t\t\tpath=f\"{data_paths}/{parquet_path}\",\n",
    "\t\t\tmode=\"overwrite\",\n",
    "\t\t\tpartitionBy=\"Date\",\n",
    "\t\t\tcompression=\"zstd\"\n",
    "\t\t)\n",
    "\t)\n",
    "\n",
    "\n",
    "read_log_content(data_path, parquet_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1abcbec63793b454"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = (\n",
    "\tspark.read.parquet(\n",
    "\t\tf\"{data_path}/{parquet_path}\"\n",
    "\t)\n",
    "\t.filter(col(\"Contract\") != \"0\")\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7566fb9748a33103"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "['2022-04-01', '2022-04-02', '2022-04-03']"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "def generate_date_range(start_date, end_date, format=\"%Y-%m-%d\"):\n",
    "  \"\"\"Generates a range of dates in string format.\n",
    "\n",
    "  Args:\n",
    "    start_date: The start date of the range.\n",
    "    end_date: The end date of the range.\n",
    "    format: The format of the dates in the range.\n",
    "\n",
    "  Returns:\n",
    "    A list of dates in string format.\n",
    "  \"\"\"\n",
    "\n",
    "  date_range = []\n",
    "  current_date = start_date\n",
    "  while current_date <= end_date:\n",
    "    date_range.append(current_date.strftime(format))\n",
    "    current_date += datetime.timedelta(days=1)\n",
    "  return date_range\n",
    "\n",
    "start_date = datetime.datetime(2022, 4, 1)\n",
    "end_date = datetime.datetime(2022, 4, 3)\n",
    "date_range = generate_date_range(start_date, end_date)\n",
    "date_range"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:55:00.017926Z",
     "start_time": "2023-10-25T13:54:59.998415Z"
    }
   },
   "id": "80a0be77d5459468"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['/Users/sonle/Documents/GitHub/spark-playground/data/parquet_log_content.parquet/Date=2022-04-01',\n '/Users/sonle/Documents/GitHub/spark-playground/data/parquet_log_content.parquet/Date=2022-04-02',\n '/Users/sonle/Documents/GitHub/spark-playground/data/parquet_log_content.parquet/Date=2022-04-03']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_file = [\n",
    "\tf\"{data_path}{parquet_path}/Date={date}\"\n",
    "\tfor date in date_range\n",
    "]\n",
    "list_file"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:00:02.899499Z",
     "start_time": "2023-10-25T14:00:02.887147Z"
    }
   },
   "id": "b5a6beeea0c80495"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Filter (3)\n",
      "+- * ColumnarToRow (2)\n",
      "   +- Scan parquet  (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [8]: [Index#32, Type#33, Id#34, Score#35L, AppName#36, Contract#37, Mac#38, TotalDuration#39L]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/sonle/Documents/GitHub/spark-playground/data/parquet_log_content.parquet/Date=2022-04-01, ... 2 entries]\n",
      "PushedFilters: [IsNotNull(Contract), Not(EqualTo(Contract,0)), Not(EqualTo(Contract,))]\n",
      "ReadSchema: struct<Index:string,Type:string,Id:string,Score:bigint,AppName:string,Contract:string,Mac:string,TotalDuration:bigint>\n",
      "\n",
      "(2) ColumnarToRow [codegen id : 1]\n",
      "Input [8]: [Index#32, Type#33, Id#34, Score#35L, AppName#36, Contract#37, Mac#38, TotalDuration#39L]\n",
      "\n",
      "(3) Filter [codegen id : 1]\n",
      "Input [8]: [Index#32, Type#33, Id#34, Score#35L, AppName#36, Contract#37, Mac#38, TotalDuration#39L]\n",
      "Condition : ((isnotnull(Contract#37) AND NOT (Contract#37 = 0)) AND NOT (Contract#37 = ))\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "\tspark.read.parquet(\n",
    "\t\t*list_file\n",
    "\t)\n",
    "\t.filter((col(\"Contract\") != \"0\") & (col(\"Contract\") != \"\"))\n",
    "\t.explain(\"formatted\")\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:00:29.512071Z",
     "start_time": "2023-10-25T14:00:28.672291Z"
    }
   },
   "id": "d95a9541486fe3fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"log_content\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bf3b546b3751a52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "edb49d62bb135136"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ETL\n",
    "The requirements at this part is that we need to extract the information of each contract in terms of total duration of five segments \"TVDuration\", \"MovieDuration\", \"SportDuration\",\n",
    "\"ChildDuration\", \"RelaxDuration\".\n",
    "\n",
    "The first option is to **aggregate sum function manually**, the second option is to use supported **pyspark pivot** method. \n",
    "\n",
    "Since we have 5 categories, we will need to rewrite some operations repeatedly which can take some time. For example:\n",
    "```python\n",
    "when(col(\"name\") == \"some_name\", value).otherwise(value)\n",
    "# or sum method in agg method\n",
    "sum(\"columns\").alias(\"columns_name\")\n",
    "```\n",
    "Thankfully, pyspark functions support arguments unpacking `*exprs` so we can leverage this by creating lists of expressions before pasting them to pyspark functions.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fde24d42ea559c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " app_names = [\n",
    "        \"CHANNEL\",\n",
    "\t\t\"KPLUS\",\n",
    "        \"VOD\",\n",
    "        \"FIMS\",\n",
    "        \"BHD\",\n",
    "        \"SPORT\",\n",
    "        \"CHILD\",\n",
    "        \"RELAX\",\n",
    "    ]\n",
    "\n",
    "column_names = [\n",
    "        \"TV\",\n",
    "        \"TV\",\n",
    "        \"Movie\",\n",
    "        \"Movie\",\n",
    "        \"Movie\",\n",
    "        \"Sport\",\n",
    "        \"Child\",\n",
    "        \"Relax\",\n",
    "    ]\n",
    "# whens = [\n",
    "# \twhen(col(\"AppName\").isin(app_name), column_name).otherwise(\"\")\n",
    "# \tfor app_name, column_name in zip(app_names, column_names)\n",
    "# ]\n",
    "# whens\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "whens_1 = F\n",
    "for app_name, column_name in zip(app_names, column_names):\n",
    "\twhens_1 = whens_1.when(col(\"AppName\") == app_name, column_name)\n",
    "whens_1 = whens_1.otherwise(\"\").alias(\"Type\")\n",
    "whens_1\n",
    "\n",
    "\t"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7eabb095e1c3ba5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def summarize_by_manual_pivot(\n",
    "\t\tdf: DataFrame,\n",
    "\t\tapp_names: list[str],\n",
    "\t\tcolumn_names: list[str]\n",
    ") -> DataFrame:\n",
    "\t\"\"\"\n",
    "\tFunction to manually pivot data. \n",
    "\t:param df: Dataframe, log content data\n",
    "\t:param app_names: application names in original data\n",
    "\t:param column_names: new column names after aggregation\n",
    "\t:return: new summarized data with required format\n",
    "\t\"\"\"\n",
    "\t\n",
    "\twhens = [when(col(\"AppName\") == app_name, col(\"TotalDuration\")).otherwise(lit(0)).alias(f\"{column_name}\")\n",
    "\t\t\t for app_name, column_name in zip(app_names, column_names)\n",
    "\t\t\t ]\n",
    "\t\n",
    "\texprs = [sum(x).alias(f\"{x}\") for x in column_names]\n",
    "\n",
    "\treturn (\n",
    "\t\tdf\n",
    "\t\t.select(\n",
    "\t\t\tcol(\"Contract\"),\n",
    "\t\t\t*whens)\n",
    "\t\t.groupby(\"Contract\")\n",
    "\t\t.agg(*exprs)\n",
    "\t\t.orderBy(\"TVDuration\", ascending=False)\n",
    "\t)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7762d4dd9edb15ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "summarize_by_manual_pivot(df, app_names=app_map, column_names=columns_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46e7034f4ae866f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "\tsqlQuery=\"\"\"\n",
    "\tselect\n",
    "\t\tdistinct  AppName\n",
    "\t\t\n",
    "\tfrom log_content \n",
    "\t\"\"\"\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49dcfeddbc30c8e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "(\n",
    "\tdf\n",
    "\t.select(\n",
    "\t\tcol(\"Contract\"),\n",
    "\t\tcol(\"TotalDuration\"),\n",
    "\t\twhens_1\n",
    "\t)\n",
    "\t.filter(\n",
    "        (col(\"Contract\") != \"0\") & (col(\"Contract\") != \"\") & (col(\"Type\") != \"Unknown\")\n",
    "\t)\n",
    "\t.groupBy(\"Contract\")\n",
    "\t.pivot(\"Type\")\n",
    "\t.sum(\"TotalDuration\")\n",
    "\t.show()\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69d4bdddb4dd680c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "whens_1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3518f97e2d07f7a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.show() # HNH433136,null,4886,null,null,2034901\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18a75a5bfab9b686"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(\n",
    "\tdf\n",
    "\t\t.groupby(\"Contract\")\n",
    "\t\t.pivot(\"Type\")\n",
    "\t\t.sum(\"TotalDuration\")\n",
    "\t\t.show(truncate=False)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "218e87893390b621"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def summarize_by_supported_pivot(\n",
    "\t\tdf: DataFrame,\n",
    "\t\tapp_names: list[str],\n",
    "\t\tcolumn_names: list[str]\n",
    ") -> DataFrame:\n",
    "\t\"\"\"\n",
    "\tFunction to pivot data using supported pyspark one\n",
    "\t:param df: \n",
    "\t:param app_names: \n",
    "\t:param column_names: \n",
    "\t:return: \n",
    "\t\"\"\"\n",
    "\texprs = [col(x).alias(f\"{y}\") for x, y in zip(app_names, column_names)]\n",
    "\n",
    "\treturn (\n",
    "\t\tdf\n",
    "\t\t.groupby(\"Contract\")\n",
    "\t\t.pivot(\"AppName\", app_names)\n",
    "\t\t.sum(\"TotalDuration\")\n",
    "\t\t.select(\n",
    "\t\t\tcol(\"Contract\"),\n",
    "\t\t\t*exprs\n",
    "\t\t)\n",
    "\t\t.orderBy(\"TVDuration\", ascending=False)\n",
    "\t)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6469543f8d7f1c2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "summarize_by_supported_pivot(df, app_names=app_names, column_names=column_names)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21753bee715fdba6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
