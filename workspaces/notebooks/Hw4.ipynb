{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Homework 4\n",
    "\n",
    "In homework 2, we will work with log content of user in 30 days. The general idea here is we first try to analyze one day\n",
    "then apply it to a month. Thanks to **pyspark**  supporting glob pattern directory, this can be done with ease. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4914d9e67d76fe97"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import ast\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import greatest\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col, to_date, when, \\\n",
    "\tinput_file_name, regexp_extract, sum, lit\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, FloatType\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T17:41:45.455602Z",
     "start_time": "2023-12-13T17:41:45.187016Z"
    }
   },
   "id": "127052a690f1333b"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1cac1382808c1e29"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandera"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T17:25:52.411704Z",
     "start_time": "2023-12-13T17:25:52.403416Z"
    }
   },
   "id": "5c65f9fb34fdd622"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/14 00:41:49 WARN Utils: Your hostname, Andrea-Le-MBP-Pro.local resolves to a loopback address: 127.0.2.2; using 192.168.1.4 instead (on interface en0)\n",
      "23/12/14 00:41:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/sonle/.sdkman/candidates/spark/3.4.0/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/sonle/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/sonle/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d2cb331e-ce16-424c-b676-93f6f93e6d59;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 273ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d2cb331e-ce16-424c-b676-93f6f93e6d59\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/6ms)\n",
      "23/12/14 00:42:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = (\n",
    "\tSparkSession.builder.appName(\"homework1\")\n",
    "\t.config(\"spark.driver.memory\", \"16g\")\n",
    "\t.config(\"spark.driver.cores\", 4)\n",
    "\t.config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "\t.config(\n",
    "\t\t\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n",
    "\t)\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T17:42:21.251204Z",
     "start_time": "2023-12-13T17:41:45.837530Z"
    }
   },
   "id": "89113c134976b87f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parquet is all the way\n",
    "Log content here is stored on disk in **json format** which is not well-suited for reading purpose. \n",
    "Hence, we should load and write it down as **parquet format** since parquet is more toward to analytical workload. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "768a2d637be73c1a"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "log_path = \"/Users/sonle/Documents/GitHub/spark-playground/data/log_content\"\n",
    "data_path = \"/Users/sonle/Documents/GitHub/spark-playground/data/\"\n",
    "parquet_path = \"parquet_log_content.parquet\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T17:42:21.261517Z",
     "start_time": "2023-12-13T17:42:21.256147Z"
    }
   },
   "id": "f295133acf4acecf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Just run this function one time\n",
    "def read_log_content(\n",
    "\t\tdata_paths: str,\n",
    "\t\tparquet_path: str\n",
    "):\n",
    "\t\"\"\"\n",
    "    Read and rename log content data in json format and \n",
    "    write it to disk in parquet format with partitioning option\n",
    "    :param data_paths: data directory\n",
    "    :param parquet_path: parquet file name\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\treturn (\n",
    "\t\tspark.read.json(f\"{data_paths}/log_content/*.json\")\n",
    "\t\t.select(\n",
    "\t\t\tcol(\"_index\").alias(\"Index\"),\n",
    "\t\t\tcol(\"_type\").alias(\"Type\"),\n",
    "\t\t\tcol(\"_id\").alias(\"Id\"),\n",
    "\t\t\tcol(\"_score\").alias(\"Score\"),\n",
    "\t\t\tcol(\"_source.*\"),\n",
    "\t\t\tto_date(\n",
    "\t\t\t\tregexp_extract(input_file_name(), r\"\\d{8}\", 0),\n",
    "\t\t\t\t\"yyyyMMdd\"\n",
    "\t\t\t).alias(\"Date\")\n",
    "\t\t)\n",
    "\t\t.write.parquet(\n",
    "\t\t\tpath=f\"{data_paths}/{parquet_path}\",\n",
    "\t\t\tmode=\"overwrite\",\n",
    "\t\t\tpartitionBy=\"Date\",\n",
    "\t\t\tcompression=\"zstd\"\n",
    "\t\t)\n",
    "\t)\n",
    "\n",
    "\n",
    "read_log_content(data_path, parquet_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T03:28:48.201460Z"
    }
   },
   "id": "1abcbec63793b454"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------------------+-----+-------+---------+------------+-------------+----------+\n",
      "|  Index| Type|                  Id|Score|AppName| Contract|         Mac|TotalDuration|      Date|\n",
      "+-------+-----+--------------------+-----+-------+---------+------------+-------------+----------+\n",
      "|history|kplus|AX_momhia1FFivsGrn9o|    0|  KPLUS|HNH579912|0C96E62FC55C|          254|2022-04-01|\n",
      "|history|kplus|AX_momhca1FFivsGrnvg|    0|  KPLUS|HUFD40665|CCEDDC333614|         1457|2022-04-01|\n",
      "|history|kplus|AX_momhaa1FFivsGrnny|    0|  KPLUS|HNH572635|B068E6A1C5F6|         2318|2022-04-01|\n",
      "|history|kplus|AX_momhca1FFivsGrnvv|    0|  KPLUS|HND141717|08674EE8D2C2|         1452|2022-04-01|\n",
      "|history|kplus|AX_momhia1FFivsGrn98|    0|  KPLUS|HNH743103|402343C25D7D|          251|2022-04-01|\n",
      "|history|kplus|AX_momg9a1FFivsGrnkS|    0|  KPLUS|HNH893773|B84DEE76D3B8|          924|2022-04-01|\n",
      "|history|kplus|AX_momhca1FFivsGrnwA|    0|  KPLUS|HND083642|B84DEE849A0F|         1444|2022-04-01|\n",
      "|history|kplus|AX_momhfa1FFivsGrn2u|    0|  KPLUS|DNFD74404|90324BB44C39|          691|2022-04-01|\n",
      "|history|kplus|AX_momhca1FFivsGrnwP|    0|  KPLUS|DTFD21200|B84DEED27709|         1436|2022-04-01|\n",
      "|history|kplus|AX_momhca1FFivsGrnwU|    0|  KPLUS|LDFD05747|0C96E6C95E53|         1434|2022-04-01|\n",
      "|history|kplus|AX_momhfa1FFivsGrn24|    0|  KPLUS|HNH063566|B84DEEDD1C85|          687|2022-04-01|\n",
      "|history|kplus|AX_momhia1FFivsGrn-W|    0|  KPLUS|HNH866786|10394E2790A5|          248|2022-04-01|\n",
      "|history|kplus|AX_momhia1FFivsGrn-a|    0|  KPLUS|NBAAA1128|10394E47C1AF|          247|2022-04-01|\n",
      "|history|kplus|AX_momhfa1FFivsGrn3J|    0|  KPLUS|HNH960439|B84DEED34371|          683|2022-04-01|\n",
      "|history|kplus|AX_momhia1FFivsGrn-k|    0|  KPLUS|HNJ035736|CCD4A1FA86A5|          246|2022-04-01|\n",
      "|history|kplus|AX_momhaa1FFivsGrnol|    0|  KPLUS|NTFD93673|B84DEEEF4763|         2288|2022-04-01|\n",
      "|history|kplus|AX_momhaa1FFivsGrnoq|    0|  KPLUS|HNJ063267|10394E172CA7|         2282|2022-04-01|\n",
      "|history|kplus|AX_momg9a1FFivsGrnlF|    0|  KPLUS|HNH790383|4CEBBD53378B|          906|2022-04-01|\n",
      "|history|kplus|AX_momhia1FFivsGrn-4|    0|  KPLUS|THFD12466|5CEA1D893E1C|          242|2022-04-01|\n",
      "|history|kplus|AX_momhia1FFivsGrn-9|    0|  KPLUS|HNH566080|802BF9E0DDC0|          242|2022-04-01|\n",
      "+-------+-----+--------------------+-----+-------+---------+------------+-------------+----------+\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "from pyspark.sql.types import StructField, LongType\n",
    "\n",
    "schema = StructType([\n",
    "\tStructField('_id', StringType(), True),\n",
    "\tStructField('_index', StringType(), True),\n",
    "\tStructField('_score', LongType(), True),\n",
    "\tStructField('_source',\n",
    "\t\t\t\tStructType([StructField('AppName', StringType(), True),\n",
    "\t\t\t\t\t\t\tStructField('Contract', StringType(), True),\n",
    "\t\t\t\t\t\t\tStructField('Mac', StringType(), True),\n",
    "\t\t\t\t\t\t\tStructField('TotalDuration', LongType(), True)\n",
    "\t\t\t\t\t\t\t]\n",
    "\t\t\t\t\t\t   ), True\n",
    "\t\t\t\t),\n",
    "\tStructField('_type', StringType(), True)]\n",
    ")\n",
    "\n",
    "(\n",
    "\n",
    "\tspark.read.json(\n",
    "\t\tpath=f\"{data_path}/log_content/*.json\", pathGlobFilter=\"*.json\", schema=schema\n",
    "\t).select(\n",
    "\t\tcol(\"_index\").alias(\"Index\"),\n",
    "\t\tcol(\"_type\").alias(\"Type\"),\n",
    "\t\tcol(\"_id\").alias(\"Id\"),\n",
    "\t\tcol(\"_score\").alias(\"Score\"),\n",
    "\t\tcol(\"_source.*\"),\n",
    "\t\tto_date(regexp_extract(input_file_name(), r\"\\d{8}\", 0), \"yyyyMMdd\").alias(\n",
    "\t\t\t\"Date\"\n",
    "\t\t),\n",
    "\t)\n",
    "\t.show()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T13:33:15.616820Z",
     "start_time": "2023-12-06T13:33:14.562068Z"
    }
   },
   "id": "f2c6be2668c57560"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "\tspark.read.parquet(\n",
    "\t\tf\"{data_path}/{parquet_path}\"\n",
    "\t)\n",
    "\t.filter(col(\"Contract\") != \"0\")\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T16:53:35.739174Z",
     "start_time": "2023-11-06T16:53:31.746292Z"
    }
   },
   "id": "7566fb9748a33103"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def generate_date_range(start_date, end_date, format=\"%Y-%m-%d\"):\n",
    "\t\"\"\"Generates a range of dates in string format.\n",
    "  \n",
    "    Args:\n",
    "      start_date: The start date of the range.\n",
    "      end_date: The end date of the range.\n",
    "      format: The format of the dates in the range.\n",
    "  \n",
    "    Returns:\n",
    "      A list of dates in string format.\n",
    "    \"\"\"\n",
    "\n",
    "\tdate_range = []\n",
    "\tcurrent_date = start_date\n",
    "\twhile current_date <= end_date:\n",
    "\t\tdate_range.append(current_date.strftime(format))\n",
    "\t\tcurrent_date += datetime.timedelta(days=1)\n",
    "\treturn date_range\n",
    "\n",
    "start_date = datetime.datetime(2022, 4, 1)\n",
    "end_date = datetime.datetime(2022, 4, 3)\n",
    "date_range = generate_date_range(start_date, end_date)\n",
    "date_range"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T03:28:48.208655Z"
    }
   },
   "id": "80a0be77d5459468"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_file = [\n",
    "\tf\"{data_path}{parquet_path}/Date={date}\"\n",
    "\tfor date in date_range\n",
    "]\n",
    "list_file"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T03:28:48.213666Z"
    }
   },
   "id": "b5a6beeea0c80495"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(\n",
    "\tspark.read.parquet(\n",
    "\t\t*list_file\n",
    "\t)\n",
    "\t.filter((col(\"Contract\") != \"0\") & (col(\"Contract\") != \"\"))\n",
    "\t.explain(\"formatted\")\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T03:28:48.216942Z"
    }
   },
   "id": "d95a9541486fe3fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"log_content\")\n",
    "#generate date range\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T03:28:48.220886Z"
    }
   },
   "id": "5bf3b546b3751a52"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ETL\n",
    "The requirements at this part is that we need to extract the information of each contract in terms of total duration of five segments \"TVDuration\", \"MovieDuration\", \"SportDuration\",\n",
    "\"ChildDuration\", \"RelaxDuration\".\n",
    "\n",
    "The first option is to **aggregate sum function manually**, the second option is to use supported **pyspark pivot** method. \n",
    "\n",
    "Since we have 5 categories, we will need to rewrite some operations repeatedly which can take some time. For example:\n",
    "```python\n",
    "when(col(\"name\") == \"some_name\", value).otherwise(value)\n",
    "# or sum method in agg method\n",
    "sum(\"columns\").alias(\"columns_name\")\n",
    "```\n",
    "Thankfully, pyspark functions support arguments unpacking `*exprs` so we can leverage this by creating lists of expressions before pasting them to pyspark functions.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fde24d42ea559c4"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    " app_names = [\n",
    "\t\"CHANNEL\",\n",
    "\t\"KPLUS\",\n",
    "\t\"VOD\",\n",
    "\t\"FIMS\",\n",
    "\t\"BHD\",\n",
    "\t\"SPORT\",\n",
    "\t\"CHILD\",\n",
    "\t\"RELAX\",\n",
    "]\n",
    "\n",
    "column_names = [\n",
    "\t\"TVDuration\",\n",
    "\t\"TVDuration\",\n",
    "\t\"MovieDuration\",\n",
    "\t\"MovieDuration\",\n",
    "\t\"MovieDuration\",\n",
    "\t\"SportDuration\",\n",
    "\t\"ChildDuration\",\n",
    "\t\"RelaxDuration\",\n",
    "]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T17:45:45.577503Z",
     "start_time": "2023-12-13T17:45:45.554373Z"
    }
   },
   "id": "7eabb095e1c3ba5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def summarize_by_manual_pivot(\n",
    "\t\tdf: DataFrame,\n",
    "\t\tapp_names: list[str],\n",
    "\t\tcolumn_names: list[str]\n",
    ") -> DataFrame:\n",
    "\t\"\"\"\n",
    "\tFunction to manually pivot data. \n",
    "\t:param df: Dataframe, log content data\n",
    "\t:param app_names: application names in original data\n",
    "\t:param column_names: new column names after aggregation\n",
    "\t:return: new summarized data with required format\n",
    "\t\"\"\"\n",
    "\n",
    "\twhens = [when(col(\"AppName\") == app_name, col(\"TotalDuration\")).otherwise(lit(0)).alias(f\"{column_name}\")\n",
    "\t\t\t for app_name, column_name in zip(app_names, column_names)\n",
    "\t\t\t ]\n",
    "\n",
    "\texprs = [sum(x).alias(f\"{x}\") for x in column_names]\n",
    "\n",
    "\treturn (\n",
    "\t\tdf\n",
    "\t\t.select(\n",
    "\t\t\tcol(\"Contract\"),\n",
    "\t\t\t*whens\n",
    "\t\t)\n",
    "\t\t.groupby(\"Contract\")\n",
    "\t\t.agg(*exprs)\n",
    "\t\t.orderBy(\"TVDuration\", ascending=False)\n",
    "\t)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T03:28:48.227790Z"
    }
   },
   "id": "7762d4dd9edb15ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "summarize_by_manual_pivot(df, app_names=app_map, column_names=columns_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T03:28:48.232359Z"
    }
   },
   "id": "46e7034f4ae866f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pivot data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d308dbb20d8419a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def pivot_data(\n",
    "\t\tdf: DataFrame,\n",
    "\t\tapp_names: list[str],\n",
    "\t\tcolumn_names: list[str],\n",
    ") -> DataFrame:\n",
    "\tif len(app_names) != len(column_names):\n",
    "\t\traise ValueError(\"The lengths of `app_names` and `column_names` must be the same.\")\n",
    "\t\n",
    "\twhens = F\n",
    "\tfor app_name, column_name in zip(app_names, column_names):\n",
    "\t\twhens = whens.when(col(\"AppName\") == app_name, column_name)\n",
    "\twhens = whens.otherwise(\"Unknown\").alias(\"Type\")\n",
    "\n",
    "\tdf = (\n",
    "\t\tdf\n",
    "\t\t.select(\n",
    "\t\t\tcol(\"Contract\"),\n",
    "\t\t\tcol(\"TotalDuration\"),\n",
    "\t\t\twhens\n",
    "\t\t)\n",
    "\t\t.filter(\n",
    "\t\t\t(col(\"Contract\") != \"0\") & (col(\"Type\") != \"Unknown\") & (col(\"TotalDuration\") > 0) \n",
    "\t\t)\n",
    "\t\t.groupBy(\"Contract\")\n",
    "\t\t.pivot(\"Type\")\n",
    "\t\t.sum(\"TotalDuration\")\n",
    "\t)\n",
    "\n",
    "\treturn df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T17:44:21.358842Z",
     "start_time": "2023-12-13T17:44:21.328884Z"
    }
   },
   "id": "6469543f8d7f1c2f"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/Users/sonle/Documents/GitHub/spark-playground/data/delta/bronze\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T17:00:33.519971Z",
     "start_time": "2023-11-06T17:00:31.824082Z"
    }
   },
   "id": "ecee8c9199e28842"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/07 00:00:38 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pivot_df = pivot_data(df, app_names, column_names)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T17:00:45.287044Z",
     "start_time": "2023-11-06T17:00:37.798040Z"
    }
   },
   "id": "7e923badb0d8471b"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-------------+-------------+-------------+----------+\n",
      "| Contract|ChildDuration|MovieDuration|RelaxDuration|SportDuration|TVDuration|\n",
      "+---------+-------------+-------------+-------------+-------------+----------+\n",
      "|TVFD16104|         null|         null|         null|         null|   1135327|\n",
      "|HYFD48206|         null|         null|         null|         null|    178706|\n",
      "|NDFD14513|         null|         null|         null|         null|    960378|\n",
      "|SLFD07508|         null|        32902|         null|         null|    593839|\n",
      "|SGH227639|       134716|         null|         null|         null|    270153|\n",
      "|HNJ070592|         null|         null|         null|         null|    141838|\n",
      "|QND047956|         null|         null|         null|         null|   1430192|\n",
      "|HNH297108|         null|         1102|          341|         null|    405295|\n",
      "|TGD014492|         null|         null|         null|         null|    389646|\n",
      "|HPH001741|         null|         null|         null|         null|    377018|\n",
      "|HNJ091276|         null|         null|         null|         null|    562568|\n",
      "|NAFD26125|         null|         null|         null|         null|    106732|\n",
      "|HBFD12548|         null|         null|         null|         null|   1614192|\n",
      "|DAFD74968|         null|         null|           39|         null|    649350|\n",
      "|HNJ121044|         null|         7543|         null|         null|    161376|\n",
      "|HNH862709|         null|          686|         null|         null|   1197100|\n",
      "|DNH073849|         null|         5621|         null|         null|    298722|\n",
      "|HNH553236|         null|         null|         null|         null|    192605|\n",
      "|HNJ123391|         null|        14864|         null|         null|    401666|\n",
      "|HUFD43829|         null|         null|         null|         null|      1505|\n",
      "+---------+-------------+-------------+-------------+-------------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pivot_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T17:00:58.270281Z",
     "start_time": "2023-11-06T17:00:45.257027Z"
    }
   },
   "id": "3eebed4cad7a4522"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "most_watch = [\"Child\", \"Movie\", \"Relax\", \"Sport\", \"TV\"]\n",
    "columns = df.columns[1:]\n",
    "whens = F\n",
    "largest_dur = F.greatest(*columns)\n",
    "for column, name in zip(columns, most_watch):\n",
    "\twhens = whens.when(col(f\"{column}\") == largest_dur, lit(f\"{column}\"))\n",
    "whens = whens.otherwise(\"Error\").alias(\"MostWatch\")\n",
    "whens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T03:28:48.243173Z"
    }
   },
   "id": "5f79f5442e2c629f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T03:28:48.246845Z"
    }
   },
   "id": "9d347f0d0d1fd409"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def most_watch(df: DataFrame):\n",
    "\t# conds = F\n",
    "\tmost_watch = [\"Child\", \"Movie\", \"Relax\", \"Sport\", \"TV\"]\n",
    "\tcolumns = df.columns[1:]\n",
    "\t\n",
    "\twhens = F\n",
    "\tlargest_dur = F.greatest(*columns)\n",
    "\tsecond_largest_dur = (\n",
    "\t\tF.when(col)\n",
    "\t)\n",
    "\tfor column, name in zip(columns, most_watch):\n",
    "\t\twhens = whens.when(col(f\"{column}\") == largest_dur, lit(f\"{name}\"))\n",
    "\twhens = whens.otherwise(\"Error\").alias(\"MostWatch\")\n",
    "\t\n",
    "\t# conds = \"when\" + \".when\".join([\"(col('\" + c + \"') == col('LargestDuration'), lit('\" + d + \"'))\" \n",
    "\t\t\t\t\t\t\t\t   # for c, d in zip(columns, most_watch)])\n",
    "\treturn (\n",
    "\t\tdf\n",
    "\t\t.fillna(0)\n",
    "\t\t.select(\n",
    "\t\t\t\"*\",\n",
    "\t\t\twhens\n",
    "\t\t)\n",
    "\t)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T03:28:48.248897Z"
    }
   },
   "id": "ac769b5c11c6e2d6"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def second_most_watch(\n",
    "\t\tdf: DataFrame\n",
    "):\n",
    "\ttype_watch = [\"Child\", \"Movie\", \"Relax\", \"Sport\", \"TV\"]\n",
    "\twatch = [\"MostWatch\", \"SecondMostWatch\", \"ThirdMostWatch\"]\n",
    "\tcolumns = df.columns[1:]\n",
    "\n",
    "\ttemp = F.sort_array(\n",
    "\t\tF.array(\n",
    "\t\t\t*[F.struct(col(c).alias('v'), F.lit(v).alias('k')\n",
    "\t\t\t\t\t   ) for c, v in zip(columns, type_watch)]\n",
    "\t\t),\n",
    "\t\tasc=False\n",
    "\t)\n",
    "\treturn (\n",
    "\t\tdf.select(\n",
    "\t\t\tdf.Contract,\n",
    "\t\t\tdf.TVDuration,\n",
    "\t\t\tdf.ChildDuration,\n",
    "\t\t\tdf.MovieDuration,\n",
    "\t\t\tdf.RelaxDuration,\n",
    "\t\t\tdf.SportDuration,\n",
    "\t\t\ttemp[0][\"k\"].alias(\"MostWatch\"),\n",
    "\t\t\twhen(temp[1][\"v\"].isNotNull(), temp[1][\"k\"]).otherwise(None).alias(\"SecondMostWatch\"),\n",
    "\t\t\twhen(temp[2][\"v\"].isNotNull(), temp[2][\"k\"]).otherwise(None).alias(\"ThirdMostWatch\"),\n",
    "\t\t\t# *[temp[i][\"k\"].alias(f\"{x}\") for i, x in zip([0, 1, 2], watch)]\n",
    "\t\t)\n",
    "\t)\n",
    "\t\n",
    "\t"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T16:56:36.039150Z",
     "start_time": "2023-11-06T16:56:36.022678Z"
    }
   },
   "id": "126a694ad002eb5b"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------+-------------+-------------+-------------+---------+---------------+--------------+\n",
      "| Contract|TVDuration|ChildDuration|MovieDuration|RelaxDuration|SportDuration|MostWatch|SecondMostWatch|ThirdMostWatch|\n",
      "+---------+----------+-------------+-------------+-------------+-------------+---------+---------------+--------------+\n",
      "|VTH013504|    655434|         null|         null|         null|         null|       TV|           null|          null|\n",
      "|HNJ121044|    484128|         null|        22629|         null|         null|       TV|          Movie|          null|\n",
      "|GLD020852|   1517946|         1749|         null|         null|         null|       TV|          Child|          null|\n",
      "|NDFD14513|   2881134|         null|         null|         null|         null|       TV|           null|          null|\n",
      "|HPD082908|   1954119|        88605|         null|         null|         null|       TV|          Child|          null|\n",
      "|CTFD19293|   2840349|         null|         null|         null|         null|       TV|           null|          null|\n",
      "|QND047956|   4290576|         null|         null|         null|         null|       TV|           null|          null|\n",
      "|SGH828851|    798909|         null|         null|         null|         3219|       TV|          Sport|          null|\n",
      "|BTFD09004|    133824|         null|         null|         null|         null|       TV|           null|          null|\n",
      "|HYFD48206|    536118|         null|         null|         null|         null|       TV|           null|          null|\n",
      "|TGD014492|   1168938|         null|         null|         null|         null|       TV|           null|          null|\n",
      "|HNH297108|   1215885|         null|         3306|         1023|         null|       TV|          Movie|         Relax|\n",
      "|HBFD12548|   4842576|         null|         null|         null|         null|       TV|           null|          null|\n",
      "|HNJ091276|   1687704|         null|         null|         null|         null|       TV|           null|          null|\n",
      "|HNJ123391|   1204998|         null|        44592|         null|         null|       TV|          Movie|          null|\n",
      "|SGH555623|    394572|         null|         null|         null|         null|       TV|           null|          null|\n",
      "|QNFD05574|   1172325|         null|       165687|         null|         null|       TV|          Movie|          null|\n",
      "|DAFD74968|   1948050|         null|         null|          117|         null|       TV|          Relax|          null|\n",
      "|PTFD24459|    798834|         null|         null|         null|         null|       TV|           null|          null|\n",
      "|BIFD38022|   1066011|          897|         7464|        43971|         null|       TV|          Relax|         Movie|\n",
      "+---------+----------+-------------+-------------+-------------+-------------+---------+---------------+--------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "second_most_watch(pivot_df).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T16:57:32.382527Z",
     "start_time": "2023-11-06T16:56:59.650118Z"
    }
   },
   "id": "ef0f3bac1c596557"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import count_distinct\n",
    "\n",
    "\n",
    "def activeness(df: DataFrame):\n",
    "\treturn (\n",
    "\t\tdf.groupBy(\"Contract\").agg(\n",
    "\t\t\t(count_distinct(col(\"Date\"))/lit(30.)).cast(FloatType()).alias(\"Activeness\")\n",
    "\t\t)\n",
    "\t)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T03:28:48.254732Z"
    }
   },
   "id": "8a4dd13eb162ee9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "activeness(df).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T03:28:48.256703Z"
    }
   },
   "id": "7dabaefc540c0459"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pipe(self, func, *args, **kwargs):\n",
    "\treturn func(self, *args, **kwargs)\n",
    "DataFrame.pipe = pipe"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T03:28:48.258656Z"
    }
   },
   "id": "c2ed4eac3f25d655"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T03:28:48.260807Z"
    }
   },
   "id": "f0548d72a0876b38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pipe_test(df):\n",
    "\tactiveness_df = calculate_rfm(df)\n",
    "\treturn \\\n",
    "\t\t(df.pipe(pivot_data, app_names, column_names)\n",
    "\t\t .pipe(second_most_watch)\n",
    "\t\t .join(activeness_df, on=\"Contract\")\n",
    "\t\t )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T03:28:48.264500Z"
    }
   },
   "id": "ec87fc45be847b70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipe_test(df).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T03:28:48.266858Z"
    }
   },
   "id": "79fa587310ba2c04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def normal_test(df):\n",
    "\tpivot_df = pivot_data(df, app_names, column_names)\n",
    "\tpivot_df = second_most_watch(pivot_df)\n",
    "\tactiveness_df = activeness(df)\n",
    "\treturn pivot_df.join(activeness_df, on=\"Contract\")\n",
    "\t"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T03:28:48.269022Z"
    }
   },
   "id": "4635acb2f5dfbfb9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%memit normal_test(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T03:28:48.271107Z"
    }
   },
   "id": "fbc4a3e8dc7429a"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max, array, array_max, datediff, min, round, ntile, count_distinct\n",
    "from pyspark.sql import Window\n",
    "\n",
    "\n",
    "def calculate_rfm(df: DataFrame):\n",
    "\tb = (\n",
    "\t\tdf.groupBy(\"Contract\").agg(\n",
    "\t\t\tmax(col(\"Date\")).alias(\"LatestDate\"),\n",
    "\t\t)\n",
    "\t)\n",
    "\n",
    "\trfm = (\n",
    "\t\tdf\n",
    "\t\t.withColumn(\"ReportedDate\", to_date(lit('20220501'), \"yyyyMMdd\"))\n",
    "\t\t.join(b, on=\"Contract\")\n",
    "\t\t.groupBy(col(\"Contract\"))\n",
    "\t\t.agg(\n",
    "\t\t\tmin(datediff(col(\"ReportedDate\"), col(\"LatestDate\"))).alias(\"Recency\"),\n",
    "\t\t\tround(count_distinct(col(\"Date\")) / lit(30) * 100, 2).alias(\"Frequency\"),\n",
    "\t\t\tsum(col(\"TotalDuration\")).alias(\"Monetary_Duration\")\n",
    "\t\t)\n",
    "\t\t.withColumns(\n",
    "\t\t\t{'Recency_score'          : ntile(4).over(Window.orderBy(\"Recency\")),\n",
    "\t\t\t 'Frequency_score'        : ntile(4).over(Window.orderBy(\"Frequency\")),\n",
    "\t\t\t 'Monetary_Duration_score': ntile(4).over(Window.orderBy(\"Monetary_Duration\"))}\n",
    "\t\t)\n",
    "\n",
    "\t)\n",
    "\treturn rfm\n",
    "\n",
    "\n",
    "\t\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T03:30:06.141598Z",
     "start_time": "2023-11-06T03:30:06.124308Z"
    }
   },
   "id": "284cea72eb089a3c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "df = (\n",
    "\tspark.read.parquet(\n",
    "\t\tf\"{data_path}{parquet_path}\"\n",
    "\t)\n",
    "\t.filter(\n",
    "\t\tcol(\"Date\").between('2022-04-01', '2022-04-05')\n",
    "\t)\n",
    "\t.filter(\n",
    "\t\t(col(\"Contract\") != \"\")\n",
    "            & (col(\"Contract\") != \"0\")\n",
    "            & (col(\"TotalDuration\") > 0)\n",
    "\t)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T03:31:39.825705Z",
     "start_time": "2023-11-06T03:31:36.125772Z"
    }
   },
   "id": "f1857581f5e0a93f"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/06 10:31:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:31:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:31:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:31:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:31:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:31:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:31:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:31:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:31:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:31:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:31:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:31:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:31:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:32:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:32:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:32:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:32:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:32:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:32:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/06 10:32:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+-----------------+-------------+---------------+-----------------------+\n",
      "| Contract|Recency|Frequency|Monetary_Duration|Recency_score|Frequency_score|Monetary_Duration_score|\n",
      "+---------+-------+---------+-----------------+-------------+---------------+-----------------------+\n",
      "|AGAAA1218|     26|     3.33|               33|            1|              1|                      1|\n",
      "|AGFD49198|     26|     3.33|               33|            1|              1|                      1|\n",
      "|BEAAA0903|     26|     3.33|               33|            1|              1|                      1|\n",
      "|BED006259|     26|     3.33|               33|            1|              1|                      1|\n",
      "|BGFD58887|     26|     3.33|               33|            1|              1|                      1|\n",
      "|BIFD07669|     26|     3.33|               33|            1|              1|                      1|\n",
      "|BIFD22405|     26|     3.33|               33|            1|              1|                      1|\n",
      "|BNFD65213|     26|     3.33|               33|            1|              1|                      1|\n",
      "|AGFD54320|     26|     3.33|               33|            1|              1|                      1|\n",
      "|BNFD71613|     26|     3.33|               33|            1|              1|                      1|\n",
      "|DLFD34434|     26|     3.33|               33|            1|              1|                      1|\n",
      "|DNH012719|     26|     3.33|               33|            1|              1|                      1|\n",
      "|DTFD14131|     26|     3.33|               33|            1|              1|                      1|\n",
      "|HNH853851|     26|     3.33|               33|            1|              1|                      1|\n",
      "|HPAAA1989|     26|     3.33|               33|            1|              1|                      1|\n",
      "|HPFD44805|     26|     3.33|               33|            1|              1|                      1|\n",
      "|KTFD01250|     26|     3.33|               33|            1|              1|                      1|\n",
      "|NDFD10262|     26|     3.33|               33|            1|              1|                      1|\n",
      "|NNFD15609|     26|     3.33|               33|            1|              1|                      1|\n",
      "|PTFD09030|     26|     3.33|               33|            1|              1|                      1|\n",
      "+---------+-------+---------+-----------------+-------------+---------------+-----------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "calculate_rfm(df).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T03:32:22.075733Z",
     "start_time": "2023-11-06T03:31:46.529492Z"
    }
   },
   "id": "5e77fc8784611bda"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `2022-05-01` cannot be resolved. Did you mean one of the following? [`AppName`, `Contract`, `Date`, `Id`, `Index`].;\n'Project [Index#0, Type#1, Id#2, Score#3L, AppName#4, Contract#5, Mac#6, TotalDuration#7L, Date#8, to_date('2022-05-01, Some(yyyyMMdd), Some(Asia/Ho_Chi_Minh)) AS ReportedDate#1528]\n+- Filter ((Date#8 >= cast(2022-04-01 as date)) AND (Date#8 <= cast(2022-04-05 as date)))\n   +- Relation [Index#0,Type#1,Id#2,Score#3L,AppName#4,Contract#5,Mac#6,TotalDuration#7L,Date#8] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdatetime\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mReportedDate\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mto_date\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m2022-05-01\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43myyyyMMdd\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/spark-playground-WomFRekb-py3.11/lib/python3.11/site-packages/pyspark/sql/dataframe.py:4789\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   4784\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n\u001B[1;32m   4785\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   4786\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   4787\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   4788\u001B[0m     )\n\u001B[0;32m-> 4789\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/spark-playground-WomFRekb-py3.11/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/spark-playground-WomFRekb-py3.11/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:175\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    171\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    173\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 175\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    177\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `2022-05-01` cannot be resolved. Did you mean one of the following? [`AppName`, `Contract`, `Date`, `Id`, `Index`].;\n'Project [Index#0, Type#1, Id#2, Score#3L, AppName#4, Contract#5, Mac#6, TotalDuration#7L, Date#8, to_date('2022-05-01, Some(yyyyMMdd), Some(Asia/Ho_Chi_Minh)) AS ReportedDate#1528]\n+- Filter ((Date#8 >= cast(2022-04-01 as date)) AND (Date#8 <= cast(2022-04-05 as date)))\n   +- Relation [Index#0,Type#1,Id#2,Score#3L,AppName#4,Contract#5,Mac#6,TotalDuration#7L,Date#8] parquet\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "df.withColumn(\"ReportedDate\", to_date(lit(datetime.datetime(2022, 5, 1)), \"yyyyMMdd\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T17:47:39.485900Z",
     "start_time": "2023-10-30T17:47:39.423457Z"
    }
   },
   "id": "6f2d14cbd2efc610"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"log_content\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T17:53:41.674626Z",
     "start_time": "2023-10-30T17:53:41.628033Z"
    }
   },
   "id": "a4109d359cf4abc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DeltaLake "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e0858558c33450e"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "table_manager = TableManager(spark)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T18:31:56.185043Z",
     "start_time": "2023-11-06T18:31:56.161332Z"
    }
   },
   "id": "ac7028b3fcf92689"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:====================================================>   (61 + 4) / 65]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Index: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Score: long (nullable = true)\n",
      " |-- AppName: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- Mac: string (nullable = true)\n",
      " |-- TotalDuration: long (nullable = true)\n",
      " |-- Date: date (nullable = true)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "\t\tspark.read.json(f\"{data_path}/log_content/*.json\")\n",
    "\t\t.select(\n",
    "\t\t\tcol(\"_index\").alias(\"Index\"),\n",
    "\t\t\tcol(\"_type\").alias(\"Type\"),\n",
    "\t\t\tcol(\"_id\").alias(\"Id\"),\n",
    "\t\t\tcol(\"_score\").alias(\"Score\"),\n",
    "\t\t\tcol(\"_source.*\"),\n",
    "\t\t\tto_date(\n",
    "\t\t\t\tregexp_extract(input_file_name(), r\"\\d{8}\", 0),\n",
    "\t\t\t\t\"yyyyMMdd\"\n",
    "\t\t\t).alias(\"Date\")\n",
    "\t\t)\n",
    "\t.filter(col(\"Date\").between('2022-04-01', '2022-04-05'))\n",
    ")\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T18:31:30.818114Z",
     "start_time": "2023-11-06T18:31:03.988161Z"
    }
   },
   "id": "5e359dd9bd7f9186"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "raw_table_schema = [\n",
    "\t(\"Index\", \"STRING\"),\n",
    "\t(\"Type\", \"STRING\"),\n",
    "\t(\"Id\", \"STRING\"),\n",
    "\t(\"Score\", \"LONG\"),\n",
    "\t(\"AppName\", \"STRING\"),\n",
    "\t(\"Contract\", \"STRING\"),\n",
    "\t(\"MAC\", \"STRING\"),\n",
    "\t(\"TotalDuration\", \"LONG\"),\n",
    "\t(\"Date\", \"DATE\"),\n",
    "]\n",
    "raw_table_path = \"/Users/sonle/Documents/GitHub/spark-playground/data/DeltaFolder/raw_table\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T18:32:39.755156Z",
     "start_time": "2023-11-06T18:32:39.734632Z"
    }
   },
   "id": "9898b4551364bc4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create, Load and Delete Table"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eaa5d347b4f5f7f4"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "<delta.tables.DeltaTable at 0x10f837210>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_manager.create_table(\"raw_table\", raw_table_schema, raw_table_path, is_partition=True, partition_by=\"Date\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T18:32:53.461061Z",
     "start_time": "2023-11-06T18:32:51.433330Z"
    }
   },
   "id": "fe8a641b194753d0"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|            Contract|   string|   null|\n",
      "|             Recency|      int|   null|\n",
      "|           Frequency|   double|   null|\n",
      "|   Monetary_Duration|   bigint|   null|\n",
      "|       Recency_Score|      int|   null|\n",
      "|     Frequency_Score|      int|   null|\n",
      "|Monetary_Duration...|      int|   null|\n",
      "+--------------------+---------+-------+\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe delta.`{}`\".format(\"/Users/sonle/Documents/GitHub/spark-playground/data/delta/silver/rfm\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T19:28:56.668470Z",
     "start_time": "2023-11-06T19:28:56.590031Z"
    }
   },
   "id": "7365f0e157dca03e"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "table_manager.load_table(df, raw_table_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T18:36:31.004238Z",
     "start_time": "2023-11-06T18:35:08.402639Z"
    }
   },
   "id": "89d333c31cb1604c"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near '''.(line 1, pos 16)\n\n== SQL ==\ndescribe detail '/Users/sonle/Documents/GitHub/spark-playground/data/delta/bronze`\n----------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mParseException\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdescribe detail \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m`\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/Users/sonle/Documents/GitHub/spark-playground/data/delta/bronze\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/spark-playground-WomFRekb-py3.11/lib/python3.11/site-packages/pyspark/sql/session.py:1440\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1438\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1439\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1440\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1441\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1442\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/spark-playground-WomFRekb-py3.11/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/spark-playground-WomFRekb-py3.11/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:175\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    171\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    173\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 175\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    177\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mParseException\u001B[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near '''.(line 1, pos 16)\n\n== SQL ==\ndescribe detail '/Users/sonle/Documents/GitHub/spark-playground/data/delta/bronze`\n----------------^^^\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe detail '{}`\".format(\"/Users/sonle/Documents/GitHub/spark-playground/data/delta/bronze\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T18:40:24.881291Z",
     "start_time": "2023-11-06T18:40:24.810278Z"
    }
   },
   "id": "1465cdd6207cb64e"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists raw_table\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T22:50:15.561089Z",
     "start_time": "2023-11-03T22:50:15.516235Z"
    }
   },
   "id": "ede71e1dee90555a"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+-----+-------+--------+---+-------------+----+\n",
      "|Index|Type| Id|Score|AppName|Contract|MAC|TotalDuration|Date|\n",
      "+-----+----+---+-----+-------+--------+---+-------------+----+\n",
      "+-----+----+---+-----+-------+--------+---+-------------+----+\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from raw_table\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T16:01:45.551477Z",
     "start_time": "2023-11-04T16:01:45.158402Z"
    }
   },
   "id": "8d13ae8f0cdc021a"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "delta_df = spark.read.format(\"delta\").load(f\"{raw_table_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T16:24:01.986984Z",
     "start_time": "2023-11-04T16:24:01.952190Z"
    }
   },
   "id": "f4dcd699527c2d60"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 90:======================================>                  (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 ms, sys: 3.96 ms, total: 14.8 ms\n",
      "Wall time: 2.89 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "pivot_delta_df = pivot_data(delta_df, app_names=app_names, column_names=column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T16:24:08.280461Z",
     "start_time": "2023-11-04T16:24:05.385144Z"
    }
   },
   "id": "6dcc8d999edbe739"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7d1f625caa79c883"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:====================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 ms, sys: 6.44 ms, total: 21 ms\n",
      "Wall time: 2.64 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "DataFrame[Contract: string, ChildDuration: bigint, MovieDuration: bigint, RelaxDuration: bigint, SportDuration: bigint, TVDuration: bigint]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pivot_data(df, app_names=app_names, column_names=column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T22:59:02.176183Z",
     "start_time": "2023-11-03T22:58:59.526763Z"
    }
   },
   "id": "3ee726ee966ff662"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from delta.tables import  DeltaTable\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, raw_table_path)\n",
    "deltaTable.delete()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T16:01:34.596342Z",
     "start_time": "2023-11-04T16:01:33.547259Z"
    }
   },
   "id": "baad4e803e7edd04"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[version: bigint, timestamp: timestamp, userId: string, userName: string, operation: string, operationParameters: map<string,string>, job: struct<jobId:string,jobName:string,runId:string,jobOwnerId:string,triggerType:string>, notebook: struct<notebookId:string>, clusterId: string, readVersion: bigint, isolationLevel: string, isBlindAppend: boolean, operationMetrics: map<string,string>, userMetadata: string, engineInfo: string]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.history()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T15:53:08.314858Z",
     "start_time": "2023-11-04T15:53:07.678544Z"
    }
   },
   "id": "8fc42f87c4a666cc"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "delta_df = (\n",
    "\tspark.read.format(\"delta\").load(f\"{data_path}{parquet_path}\")\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T22:16:21.815103Z",
     "start_time": "2023-11-03T22:16:15.802553Z"
    }
   },
   "id": "9a9da12c15c382b3"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "parquet_df = df = (\n",
    "\tspark.read.parquet(\n",
    "\t\tf\"{data_path}{parquet_path}\"\n",
    "\t)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T22:16:43.112112Z",
     "start_time": "2023-11-03T22:16:42.789977Z"
    }
   },
   "id": "602fd1678068a3d0"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:====================================================>   (26 + 2) / 28]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.5 ms, sys: 8.88 ms, total: 32.4 ms\n",
      "Wall time: 10.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "DataFrame[Contract: string, ChildDuration: bigint, MovieDuration: bigint, RelaxDuration: bigint, SportDuration: bigint, TVDuration: bigint]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pivot_data(parquet_df, app_names=app_names, column_names=column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T22:17:08.152559Z",
     "start_time": "2023-11-03T22:16:57.306217Z"
    }
   },
   "id": "86ec30a5ecfdd2c0"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint>]"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.optimize().executeCompaction()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T16:27:18.708289Z",
     "start_time": "2023-11-03T16:26:33.951573Z"
    }
   },
   "id": "a038e465482a8632"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint>]"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.optimize().executeZOrderBy(\"Type\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T16:31:53.922570Z",
     "start_time": "2023-11-03T16:30:34.420605Z"
    }
   },
   "id": "873d7cfc36e5d139"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1ac1be0a5371180b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validation with Pandera"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b746eaea1416420f"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Index: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Score: long (nullable = true)\n",
      " |-- AppName: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- Mac: string (nullable = true)\n",
      " |-- TotalDuration: long (nullable = true)\n",
      " |-- Date: date (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "\tspark.read.parquet(\n",
    "\t\tf\"{data_path}{parquet_path}\"\n",
    "\t)\n",
    "\t.filter(\n",
    "\t\tcol(\"Date\").between('2022-04-01', '2022-04-12')\n",
    "\t)\n",
    ")\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T17:50:25.511879Z",
     "start_time": "2023-12-13T17:50:25.185913Z"
    }
   },
   "id": "8f1077a1c7afc043"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pivot_df = pivot_data(df, app_names=app_names, column_names=column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T17:52:07.679969Z",
     "start_time": "2023-12-13T17:51:55.424857Z"
    }
   },
   "id": "71616c63e627f9d1"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sonle/Library/Caches/pypoetry/virtualenvs/spark-playground-WomFRekb-py3.11/lib/python3.11/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "import pandera.pyspark as pa\n",
    "\n",
    "from pandera.pyspark import DataFrameModel\n",
    "\n",
    "class PivotSchema(DataFrameModel):\n",
    "\tContract: StringType = pa.Field(nullable=True)\n",
    "\tMovieDuration: LongType = pa.Field(nullable=True)\n",
    "\tChildDuration: LongType = pa.Field(nullable=True)\n",
    "\tRelaxDuration: LongType = pa.Field(nullable=True)\n",
    "\tSportDuration: LongType = pa.Field(nullable=True)\n",
    "\tTVDuration: LongType = pa.Field(nullable=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T17:44:49.566592Z",
     "start_time": "2023-12-13T17:44:47.238795Z"
    }
   },
   "id": "f31451d5d51df845"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PANDERA_CACHE_DATAFRAME=False\n"
     ]
    }
   ],
   "source": [
    "%env PANDERA_CACHE_DATAFRAME=False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T17:51:45.083357Z",
     "start_time": "2023-12-13T17:51:44.993609Z"
    }
   },
   "id": "7aff5e8a2ad33c0c"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "val_df_false = PivotSchema.validate(pivot_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T17:53:24.729422Z",
     "start_time": "2023-12-13T17:53:24.672593Z"
    }
   },
   "id": "cda4957122dd21b0"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PANDERA_CACHE_DATAFRAME=True\n"
     ]
    }
   ],
   "source": [
    "%env PANDERA_CACHE_DATAFRAME=True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T17:53:52.642782Z",
     "start_time": "2023-12-13T17:53:52.602863Z"
    }
   },
   "id": "79d40777e68816e9"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "val_df_true = PivotSchema.validate(pivot_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T17:53:54.139581Z",
     "start_time": "2023-12-13T17:53:54.107544Z"
    }
   },
   "id": "b495b37b72b3ccc8"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "error = val_df.pandera.errors\n",
    "if error is None:\n",
    "\tprint(error)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T17:51:29.160726Z",
     "start_time": "2023-12-13T17:51:29.102135Z"
    }
   },
   "id": "e5db717f69cdf8fa"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "validated_schema = {\n",
    "\t\"pivot\" : PivotSchema\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T19:18:39.501559Z",
     "start_time": "2023-11-05T19:18:39.474723Z"
    }
   },
   "id": "cb12980691684ab6"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "PivotSchema"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validated_schema.get(\"pivot\").validate(pivot_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T19:18:50.087433Z",
     "start_time": "2023-11-05T19:18:50.066728Z"
    }
   },
   "id": "35dab6464262c3b5"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "publish = False\n",
    "if publish:\n",
    "\tprint(\"Publishing\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T18:17:00.260562Z",
     "start_time": "2023-11-05T18:17:00.244286Z"
    }
   },
   "id": "21843d5e233e2c38"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
